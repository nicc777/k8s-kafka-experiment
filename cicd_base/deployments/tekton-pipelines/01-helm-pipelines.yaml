
# PERMISSIONS
apiVersion: v1
kind: ServiceAccount
metadata:
  name: helm-pipelines-sa
  annotations:
    argocd.argoproj.io/sync-wave: "1"
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: helm-pipelines-eventlistener-binding
  annotations:
    argocd.argoproj.io/sync-wave: "1"
subjects:
- kind: ServiceAccount
  name: helm-pipelines-sa
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: tekton-triggers-eventlistener-roles
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: helm-pipeline-eventlistener-clusterbinding
  annotations:
    argocd.argoproj.io/sync-wave: "1"
subjects:
- kind: ServiceAccount
  name: helm-pipelines-sa
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: tekton-triggers-eventlistener-clusterroles

# WEBHOOK....
---
  apiVersion: triggers.tekton.dev/v1beta1
  kind: EventListener
  metadata:
    name: helm-pipelines-event-listener
  spec:
    triggers:
    - name: helm-pipelines-event-listener
      interceptors: 
      - name: "Command"
        ref:
          name: "cel"
        params:
        - name: "filter"
          value: "body.command in ['apply', 'delete']"
      bindings:
        - ref: helm-pipelines-trigger-binding
      template:
        ref: helm-pipelines-trigger-template
    resources:
      kubernetesResource:
        spec:
          template:
            spec:
              serviceAccountName: helm-pipelines-sa
              containers:
              - resources:
                  requests:
                    memory: "64Mi"
                    cpu: "250m"
                  limits:
                    memory: "128Mi"
                    cpu: "500m"
---
apiVersion: triggers.tekton.dev/v1beta1
kind: TriggerBinding
metadata:
  name: helm-pipelines-trigger-binding
spec:
  params: 
  - name: command
    value: $(body.command)
---
apiVersion: triggers.tekton.dev/v1beta1
kind: TriggerTemplate
metadata:
  name: helm-pipelines-trigger-template
spec:
  params:
  - name: command
  resourcetemplates:
  - apiVersion: tekton.dev/v1beta1
    kind: TaskRun
    metadata:
      generateName: helm-pipelines-run-
    spec:
      taskSpec:
        volumes:
        - name: apply-volume
          configMap:
            name: helm-pipelines-apply-script-cm
        - name: delete-volume
          configMap:
            name: helm-pipelines-delete-script-cm
        - name: valkey-values-volume
          configMap:
            name: helm-pipelines-valkey-values-cm
        - name: kafka-ui-values-volume
          configMap:
            name: helm-pipelines-kafka-ui-values-cm
        steps:
        - image: ghcr.io/nicc777/container-python4aws:v3.12.3-3
          name: manage-base-resources
          script: |
            #!/bin/bash
            export DEBIAN_FRONTEND=noninteractive
            echo "I am running as user " `whoami`
            cd /tmp
            git clone https://github.com/nicc777/k8s-kafka-experiment.git
            cd k8s-kafka-experiment
            sh /scripts/$(tt.params.command)/exec.sh || true
          volumeMounts:
          - name: apply-volume
            mountPath: /scripts/apply
          - name: delete-volume
            mountPath: /scripts/delete
          - name: valkey-values-volume
            mountPath: /data/custom-helm-values/valkey
          - name: kafka-ui-values-volume
            mountPath: /data/custom-helm-values/kafka-ui

---
# FIXME
# Current error (only on CIVO, not on Microk8s): 
#   2024-10-26T15:13:00.556984628Z Error: INSTALLATION FAILED: Unable to continue with install: could not get information about the resource NetworkPolicy "valkey" in namespace "exp": networkpolicies.networking.k8s.io "valkey" is forbidden: User "system:serviceaccount:default:default" cannot get resource "networkpolicies" in API group "networking.k8s.io" in the namespace "exp"
# It seems this error is related to permissions and additional controls based on the platform.
apiVersion: v1
kind: ConfigMap
metadata:
  name: helm-pipelines-apply-script-cm
data:
  exec.sh: |
    echo "I AM AN APPLY SCRIPT"
    helm version
    #!/bin/bash
    cd /tmp/k8s-kafka-experiment
    export NAMESPACE="exp"
    # VALKEY
    export VALKEY_RELEASE_NAME="valkey"
    export VALKEY_CHART_URI="oci://registry-1.docker.io/bitnamicharts/valkey"
    if helm ls -n $NAMESPACE --filter ^$VALKEY_RELEASE_NAME$ | grep -q $VALKEY_RELEASE_NAME; then
      echo "Helm release $VALKEY_RELEASE_NAME already exists in namespace $NAMESPACE."
    else
      echo "Installing Helm chart $VALKEY_CHART_URI as release $VALKEY_RELEASE_NAME in namespace $NAMESPACE."
      kubectl create namespace $NAMESPACE
      helm install -f /data/custom-helm-values/valkey/values.yaml $VALKEY_RELEASE_NAME $VALKEY_CHART_URI --namespace $NAMESPACE
    fi
    # KAFKA
    helm repo add confluentinc https://packages.confluent.io/helm
    helm repo update
    export KAFKA_RELEASE_NAME="confluent-operator"
    export KAFKA_CHART_URI="confluentinc/confluent-for-kubernetes"
    if helm ls -n confluent --filter ^$KAFKA_RELEASE_NAME$ | grep -q $KAFKA_RELEASE_NAME; then
      echo "Helm release $KAFKA_RELEASE_NAME already exists in namespace confluent."
    else
      echo "Installing Helm chart $KAFKA_CHART_URI as release $KAFKA_RELEASE_NAME in namespace confluent."
      kubectl create namespace confluent
      helm upgrade --install $KAFKA_RELEASE_NAME $KAFKA_CHART_URI --namespace confluent
      sleep 5
      # Get the controller pod
      export KAFKA_CONTROLLER_POD_NAME=$(kubectl get pods -n confluent --no-headers -o custom-columns=":metadata.name" | grep "confluent-operator-")
      echo "Controller Pod Name: $KAFKA_CONTROLLER_POD_NAME"
      # Install the Kafka cluster
      kubectl wait --for=condition=ready pod/$KAFKA_CONTROLLER_POD_NAME -n confluent --timeout=300s
      if [ $? -eq 0 ]; then
          echo "Pod is running and ready"
          kubectl apply -f ./experiments/kafka-platform-deployments/application/kafka-platform.yaml
          echo "Going to sleep for 2 minutes while we wait for kafka to come up"
          sleep 120
          kubectl wait --for=condition=ready pod/kafka-0 -n confluent --timeout=300s
          if [ $? -eq 0 ]; then
            helm repo add kafka-ui https://provectus.github.io/kafka-ui-charts
            helm repo update
            helm upgrade --install -n exp \
              -f /data/custom-helm-values/kafka-ui/values.yaml \
              kafka-ui kafka-ui/kafka-ui
          else
              echo "Timed out waiting for Kafka Services"
              exit 1
          fi          
      else
          echo "Timed out waiting for Kafka Controller"
          exit 1
      fi
    fi
    echo "TASK DONE"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: helm-pipelines-delete-script-cm
data:
  exec.sh: |
    echo "I AM AN DELETE SCRIPT"
    cd /tmp/k8s-kafka-experiment
    kubectl delete -f ./experiments/kafka-platform-deployments/application/kafka-platform.yaml
    echo "Going to sleep for 2 minutes while we wait for kafka to uninstall before we delete namespaces"
    helm version
    helm repo add confluentinc https://packages.confluent.io/helm
    helm repo add kafka-ui https://provectus.github.io/kafka-ui-charts
    helm repo update 
    helm delete -n exp kafka-ui
    helm delete -n exp valkey
    helm delete -n confluent confluent-operator
    echo "Going to sleep for 2 minutes while we wait for kafka-operator to uninstall before we delete namespaces"
    sleep 120
    kubectl delete namespace exp
    kubectl delete namespace confluent
    echo "All commands executed"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: helm-pipelines-valkey-values-cm
data:
  values.yaml: |
    auth:
      enabled: false
      sentinel: false
    replica:
      replicaCount: 1
    metrics:
      enabled: true
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: helm-pipelines-kafka-ui-values-cm
data:
  values.yaml: |
    yamlApplicationConfig:
      kafka:
        clusters:
        - name: exp
          bootstrapServers: kafka.confluent.svc.cluster.local:9092
          # schemaRegistry: http://confluent.svc.cluster.local:8085
      auth:
        type: disabled
      management:
        health:
          ldap:
            enabled: false





